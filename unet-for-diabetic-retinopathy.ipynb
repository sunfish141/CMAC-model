{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":886249,"sourceType":"datasetVersion","datasetId":472549},{"sourceId":952963,"sourceType":"datasetVersion","datasetId":505422},{"sourceId":10958458,"sourceType":"datasetVersion","datasetId":6817342},{"sourceId":11123329,"sourceType":"datasetVersion","datasetId":6936566},{"sourceId":13698833,"sourceType":"datasetVersion","datasetId":8713848}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**If u like this notebook pls upvote!!!!**","metadata":{}},{"cell_type":"code","source":"#Install required modules\n\n!pip install segmentation-models-pytorch\n!pip install albumentations\n!pip install opencv-python-headless","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:06:14.216349Z","iopub.execute_input":"2025-11-14T03:06:14.216626Z","iopub.status.idle":"2025-11-14T03:07:40.407310Z","shell.execute_reply.started":"2025-11-14T03:06:14.216598Z","shell.execute_reply":"2025-11-14T03:07:40.406565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Some of these matter, some of these dont. Most of them probably dont cuz I copied these from someone else. \n#You can remove these if u want idk why u would tho\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport pandas as pd\nimport os\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, UpSampling2D, Input, Concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\nfrom keras.losses import CategoricalCrossentropy\nfrom keras.metrics import MeanIoU","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:07:40.408427Z","iopub.execute_input":"2025-11-14T03:07:40.408712Z","iopub.status.idle":"2025-11-14T03:08:00.450503Z","shell.execute_reply.started":"2025-11-14T03:07:40.408688Z","shell.execute_reply":"2025-11-14T03:08:00.449882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create the base data frame for the data. IF you have multiple files you will need to change base directory.\nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\ndef Create_Directory_DataFrame(typ='train', dataset_type='idrid'):\n    \"\"\"\n    Create DataFrame for either IDRiD or DDR dataset\n    \"\"\"\n    rows = []\n    \n    if dataset_type == 'idrid':\n        # IDRiD dataset structure\n        basedir = '/kaggle/input/indian-diabetic-retinopathy-image-dataset/A.%20Segmentation/A. Segmentation/1. Original Images/'\n        \n        for Class in os.listdir(basedir):\n            for location in os.listdir(os.path.join(basedir, Class)):\n                if location.endswith('.jpg'):\n                    class_split = Class.split(' ')\n                    class_name = class_split[1] if len(class_split) > 1 else class_split[0]\n                    \n                    rows.append({\n                        'Dataset': 'idrid',\n                        'Class': class_name,\n                        'Location': os.path.join(basedir, Class, location),\n                        'ID': location.split('.')[0].split('_')[1] if '_' in location else location.split('.')[0],\n                        'Type': 'train'  # IDRiD doesn't have separate validation, we'll split it\n                    })\n    \n    elif dataset_type == 'ddr':\n        # DDR dataset structure - fixed the f-string issue\n        basedir = f\"/kaggle/input/ddr-segmentation/lesion_segmentation/{typ}/\"\n        image_dir = os.path.join(basedir, 'image')\n        \n        if not os.path.exists(image_dir):\n            print(f\"Warning: DDR {typ} image directory not found: {image_dir}\")\n            return pd.DataFrame(rows)\n        \n        # Get all JPEG images\n        for img_path in glob(os.path.join(image_dir, '*.jpg')):\n            img_name = os.path.basename(img_path)\n            img_id = img_name.split('.')[0]  # Remove .jpeg extension\n            \n            rows.append({\n                'Dataset': 'ddr',\n                'Class': 'DR',  # DDR doesn't have separate classes for images\n                'Location': img_path,\n                'ID': img_id,\n                'Type': typ\n            })\n    \n    df = pd.DataFrame(rows)\n    if len(df) > 0:\n        df = df.sample(frac=1).reset_index(drop=True)\n    return df\n\ndef Create_Directory_Array(typ='train', dataset_type='idrid'):\n    \"\"\"\n    Create mask DataFrame for either IDRiD or DDR dataset\n    \"\"\"\n    rows = []\n    \n    if dataset_type == 'idrid':\n        # IDRiD mask structure\n        basedir = '/kaggle/input/indian-diabetic-retinopathy-image-dataset/A.%20Segmentation/A. Segmentation/2. All Segmentation Groundtruths/'\n        \n        for Class in os.listdir(basedir):\n            class_path = os.path.join(basedir, Class)\n            \n            if os.path.isdir(class_path):\n                class_parts = Class.split(' ')\n                class_label = class_parts[1] if len(class_parts) > 1 else class_parts[0]\n                \n                for subclass in os.listdir(class_path):\n                    subclass_path = os.path.join(class_path, subclass)\n                    \n                    if os.path.isdir(subclass_path):\n                        for location in os.listdir(subclass_path):\n                            if location.endswith('.tif'):\n                                rows.append({\n                                    'Dataset': 'idrid',\n                                    'Class': class_label,\n                                    'SubClass': subclass,\n                                    'Location': os.path.join(subclass_path, location),\n                                    'ID': location.split('.')[0].split('_')[1] if '_' in location else location.split('.')[0],\n                                    'Type': 'train'\n                                })\n    \n    elif dataset_type == 'ddr':\n        # DDR mask structure - now supports both train and validation\n        basedir = f\"/kaggle/input/ddr-segmentation/lesion_segmentation/{typ}/label/\"\n        if typ == 'valid':\n            basedir = f\"/kaggle/input/ddr-segmentation/lesion_segmentation/{typ}/segmentation label/\"\n        \n        if not os.path.exists(basedir):\n            print(f\"Warning: DDR {typ} label directory not found: {basedir}\")\n            return rows\n            \n        lesion_types = ['MA', 'EX', 'HE', 'SE']  # Microaneurysms, Exudates, Haemorrhages, Soft Exudates\n        \n        for lesion_type in lesion_types:\n            lesion_dir = os.path.join(basedir, lesion_type)\n            if not os.path.exists(lesion_dir):\n                print(f\"Warning: DDR {typ} lesion directory not found: {lesion_dir}\")\n                continue\n                \n            for mask_path in glob(os.path.join(lesion_dir, '*.tif')):\n                mask_name = os.path.basename(mask_path)\n                mask_id = mask_name.split('.')[0]  # Remove .tif extension\n                \n                # Map DDR lesion types to our standard class names\n                class_mapping = {\n                    'MA': 'Microaneurysms',\n                    'EX': 'Hard Exudates', \n                    'HE': 'Haemorrhages',\n                    'SE': 'Soft Exudates'\n                }\n                \n                rows.append({\n                    'Dataset': 'ddr',\n                    'Class': class_mapping[lesion_type],\n                    'SubClass': class_mapping[lesion_type],\n                    'Location': mask_path,\n                    'ID': mask_id,\n                    'Type': typ\n                })\n    \n    return rows\n\n# Let's debug step by step\nprint(\"=== Creating DataFrames ===\")\n\n# Create DataFrames for both datasets and both types\nprint(\"Creating IDRiD original DataFrame...\")\ndf_original_idrid = Create_Directory_DataFrame('train', 'idrid')\nprint(f\"IDRiD original shape: {df_original_idrid.shape}\")\n\nprint(\"\\nCreating DDR train original DataFrame...\")\ndf_original_ddr_train = Create_Directory_DataFrame('train', 'ddr')\nprint(f\"DDR train original shape: {df_original_ddr_train.shape}\")\n\nprint(\"\\nCreating DDR validation original DataFrame...\")\ndf_original_ddr_val = Create_Directory_DataFrame('valid', 'ddr')\nprint(f\"DDR validation original shape: {df_original_ddr_val.shape}\")\n\n# Create mask DataFrames for both datasets and both types\nprint(\"\\nCreating IDRiD mask array...\")\nmask_array_idrid = Create_Directory_Array('train', 'idrid')\ndf_mask_idrid = pd.DataFrame(mask_array_idrid)\nprint(f\"IDRiD mask shape: {df_mask_idrid.shape}\")\n\nprint(\"\\nCreating DDR train mask array...\")\nmask_array_ddr_train = Create_Directory_Array('train', 'ddr')\ndf_mask_ddr_train = pd.DataFrame(mask_array_ddr_train)\nprint(f\"DDR train mask shape: {df_mask_ddr_train.shape}\")\n\nprint(\"\\nCreating DDR validation mask array...\")\nmask_array_ddr_val = Create_Directory_Array('valid', 'ddr')\ndf_mask_ddr_val = pd.DataFrame(mask_array_ddr_val)\nprint(f\"DDR validation mask shape: {df_mask_ddr_val.shape}\")\n\n# Merge function for any dataset and type\ndef merge_datasets(original_df, mask_df, dataset_name):\n    \"\"\"Helper function to merge original and mask dataframes\"\"\"\n    if len(original_df) > 0 and len(mask_df) > 0 and 'ID' in original_df.columns and 'ID' in mask_df.columns:\n        print(f\"Merging {dataset_name} data...\")\n        joined = pd.merge(original_df, mask_df, on='ID', suffixes=('_img', '_mask'))\n        joined = joined.rename(columns={\n            'Location_img': 'ImagePath',\n            'Location_mask': 'MaskPath', \n            'Class_img': 'ImageClass',\n            'Class_mask': 'MaskClass',\n            'Dataset_img': 'Dataset',\n            'Type_img': 'Type'\n        })\n        # Keep only necessary columns\n        joined = joined[['ID', 'ImagePath', 'MaskPath', 'SubClass', 'Dataset', 'MaskClass', 'Type']]\n        \n        # Clean subclass for IDRiD if needed\n        if dataset_name == 'IDRiD':\n            # Clean IDRiD subclass names\n            joined['SubClassClean'] = joined['SubClass'].apply(\n                lambda x: x.split('. ')[1].strip() if '. ' in str(x) else str(x)\n            )\n        else:\n            # DDR already clean\n            joined['SubClassClean'] = joined['SubClass']\n        \n        print(f\"Joined {dataset_name} shape: {joined.shape}\")\n        return joined\n    else:\n        print(f\"Skipping {dataset_name} merge - missing data or ID column\")\n        return pd.DataFrame()\n\nprint(\"\\n=== Merging DataFrames ===\")\n\n# Merge all datasets\njoined_idrid = merge_datasets(df_original_idrid, df_mask_idrid, 'IDRiD')\njoined_ddr_train = merge_datasets(df_original_ddr_train, df_mask_ddr_train, 'DDR Train')\njoined_ddr_val = merge_datasets(df_original_ddr_val, df_mask_ddr_val, 'DDR Validation')\n\n# Combine all datasets\nall_datasets = []\nif len(joined_idrid) > 0:\n    all_datasets.append(joined_idrid)\nif len(joined_ddr_train) > 0:\n    all_datasets.append(joined_ddr_train)\nif len(joined_ddr_val) > 0:\n    all_datasets.append(joined_ddr_val)\n\nif all_datasets:\n    combined_df = pd.concat(all_datasets, ignore_index=True)\n    print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n    \n    # Show distribution by dataset and type\n    print(\"\\nDataset distribution:\")\n    print(combined_df.groupby(['Dataset', 'Type']).size())\n    \n    print(\"\\nClass distribution:\")\n    print(combined_df['MaskClass'].value_counts())\nelse:\n    combined_df = pd.DataFrame()\n    print(\"No datasets were successfully merged\")\n\nprint(\"\\n=== Final DataFrames ===\")\nprint(f\"Combined dataset: {combined_df.shape if len(combined_df) > 0 else 'Empty'}\")\n\n# Save the datasets\nif len(joined_idrid) > 0:\n    joined_idrid.to_csv('joined_idrid.csv', index=False)\n    print(\"Saved joined_idrid.csv\")\n\nif len(joined_ddr_train) > 0:\n    joined_ddr_train.to_csv('joined_ddr_train.csv', index=False)\n    print(\"Saved joined_ddr_train.csv\")\n\nif len(joined_ddr_val) > 0:\n    joined_ddr_val.to_csv('joined_ddr_val.csv', index=False)\n    print(\"Saved joined_ddr_val.csv\")\n\nif len(combined_df) > 0:\n    combined_df.to_csv('combined_dataset.csv', index=False)\n    print(\"Saved combined_dataset.csv\")\n\n# Now create train/val splits properly\ndef create_final_splits(combined_df, test_size=0.3, random_state=42):\n    \"\"\"\n    Create proper train/validation splits considering the existing splits\n    \"\"\"\n    # For IDRiD, we need to split since it doesn't have predefined validation\n    idrid_data = combined_df[combined_df['Dataset'] == 'idrid']\n    ddr_train = combined_df[(combined_df['Dataset'] == 'ddr') & (combined_df['Type'] == 'train')]\n    ddr_val = combined_df[(combined_df['Dataset'] == 'ddr') & (combined_df['Type'] == 'validation')]\n    \n    print(f\"\\nCreating final splits:\")\n    print(f\"IDRiD data: {len(idrid_data['ID'].unique())} images\")\n    print(f\"DDR train: {len(ddr_train['ID'].unique())} images\") \n    print(f\"DDR validation: {len(ddr_val['ID'].unique())} images\")\n    \n    # Split IDRiD into train/val\n    idrid_ids = idrid_data['ID'].unique()\n    idrid_train_ids, idrid_val_ids = train_test_split(\n        idrid_ids, test_size=test_size, random_state=random_state, shuffle=True\n    )\n    \n    # Create final DataFrames\n    train_df = pd.concat([\n        idrid_data[idrid_data['ID'].isin(idrid_train_ids)],\n        ddr_train\n    ], ignore_index=True)\n    \n    val_df = pd.concat([\n        idrid_data[idrid_data['ID'].isin(idrid_val_ids)],\n        ddr_val\n    ], ignore_index=True)\n    \n    print(f\"\\nFinal splits:\")\n    print(f\"Training: {len(train_df['ID'].unique())} images\")\n    print(f\"Validation: {len(val_df['ID'].unique())} images\")\n    \n    return train_df, val_df\n\n# Create final splits\nif len(combined_df) > 0:\n    train_df, val_df = create_final_splits(combined_df)\n    \n    # Save the final splits\n    train_df.to_csv('final_train_dataset.csv', index=False)\n    val_df.to_csv('final_val_dataset.csv', index=False)\n    print(\"\\nSaved final_train_dataset.csv and final_val_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:00.451965Z","iopub.execute_input":"2025-11-14T03:08:00.452470Z","iopub.status.idle":"2025-11-14T03:08:01.130334Z","shell.execute_reply.started":"2025-11-14T03:08:00.452451Z","shell.execute_reply":"2025-11-14T03:08:01.129679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Just for visualization\nimport cv2\n\n# Define class colors (same as earlier)\nCLASS_COLORS = {\n    \"MA\": (0, 0, 255),    # Red\n    \"HE\": (0, 255, 0),    # Green\n    \"EX\": (255, 0, 0),    # Blue\n    \"SE\": (255, 255, 0),  # Cyan\n    \"OD\": (255, 0, 255)   # Magenta\n}\n\nCLASS_NAMES = [\"MA\", \"HE\", \"EX\", \"SE\", \"OD\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.131038Z","iopub.execute_input":"2025-11-14T03:08:01.131294Z","iopub.status.idle":"2025-11-14T03:08:01.135651Z","shell.execute_reply.started":"2025-11-14T03:08:01.131276Z","shell.execute_reply":"2025-11-14T03:08:01.134921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rgb_to_class(mask_rgb):\n    \"\"\"\n    Convert an RGB mask image to integer mask with class IDs.\n    \n    Args:\n        mask_rgb (np.array): HxWx3 RGB mask\n    Returns:\n        mask_class (np.array): HxW integer mask (0=background, 1=MA, 2=HE, etc.)\n    \"\"\"\n    mask_class = np.zeros(mask_rgb.shape[:2], dtype=np.uint8)  # initialize background = 0\n    for idx, class_name in enumerate(CLASS_NAMES, start=1):  # IDs start at 1\n        color = CLASS_COLORS[class_name]\n        matches = np.all(mask_rgb == color, axis=-1)\n        mask_class[matches] = idx\n    return mask_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.136341Z","iopub.execute_input":"2025-11-14T03:08:01.136591Z","iopub.status.idle":"2025-11-14T03:08:01.851349Z","shell.execute_reply.started":"2025-11-14T03:08:01.136568Z","shell.execute_reply":"2025-11-14T03:08:01.850535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Defines a function named Create_Directory_Array, which is used to read metadata of TIFF image files from a specified directory structure (including class label, subclass name, full file path, and an ID extracted from the filename). These pieces of information are stored as a list of dictionaries. The list is then converted into a Pandas DataFrame, the rows of the DataFrame are randomly shuffled, and finally the DataFrame is saved as a CSV file.\n\nPrints the shape of the saved dataset and displays the first 20 rows for inspection.\n\nPerforms a grouped count (aggregation) based on the class type.","metadata":{}},{"cell_type":"code","source":"#Wow look the data is so cool amirite\nprint(train_df.shape)\ntrain_df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.852195Z","iopub.execute_input":"2025-11-14T03:08:01.852451Z","iopub.status.idle":"2025-11-14T03:08:01.890942Z","shell.execute_reply.started":"2025-11-14T03:08:01.852432Z","shell.execute_reply":"2025-11-14T03:08:01.890323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Now we got the REAL class names\nCLASS_NAMES = [\"Microaneurysms\", \"Haemorrhages\", \"Hard Exudates\", \"Soft Exudates\"]\nn_classes = len(CLASS_NAMES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.891657Z","iopub.execute_input":"2025-11-14T03:08:01.891857Z","iopub.status.idle":"2025-11-14T03:08:01.895735Z","shell.execute_reply.started":"2025-11-14T03:08:01.891842Z","shell.execute_reply":"2025-11-14T03:08:01.895078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_df[train_df['SubClassClean'].isna()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.897893Z","iopub.execute_input":"2025-11-14T03:08:01.898161Z","iopub.status.idle":"2025-11-14T03:08:01.915715Z","shell.execute_reply.started":"2025-11-14T03:08:01.898128Z","shell.execute_reply":"2025-11-14T03:08:01.914888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#PRetty self explanatory if u dont get it just slam it into chatgpt\n\ndef load_mask(mask_path, subclass_name, size=(512,512)):\n\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    mask = cv2.resize(mask, size, interpolation=cv2.INTER_NEAREST)\n    mask_class = np.zeros(mask.shape, dtype=np.uint8)\n    if np.max(mask) > 0:\n        class_id = CLASS_NAMES.index(subclass_name) + 1  # 1-based class ID\n        mask_class[mask > 0] = class_id\n        \n    mask_class = np.expand_dims(mask_class, axis=-1)\n    return mask_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.916459Z","iopub.execute_input":"2025-11-14T03:08:01.916731Z","iopub.status.idle":"2025-11-14T03:08:01.932375Z","shell.execute_reply.started":"2025-11-14T03:08:01.916704Z","shell.execute_reply":"2025-11-14T03:08:01.931739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Same as above\n\ndef load_img(image_path, size=(512,512)):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, size)\n    img = img.astype(np.float32) / 255.0\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.933126Z","iopub.execute_input":"2025-11-14T03:08:01.933417Z","iopub.status.idle":"2025-11-14T03:08:01.948060Z","shell.execute_reply.started":"2025-11-14T03:08:01.933394Z","shell.execute_reply":"2025-11-14T03:08:01.947193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This one combines the masks so we can overlay onto the image(The masks are stored as seperate images)\n\ndef combine_masks(df_group, size=(512,512)):\n    combined_mask = np.zeros((size[1], size[0], 1), dtype=np.uint8)\n    for _, row in df_group.iterrows():\n        if row['SubClassClean'] == \"Optic Disc\":\n            continue\n        single_mask = load_mask(row['MaskPath'], row['SubClassClean'], size=size)\n        combined_mask = np.maximum(combined_mask, single_mask)\n    return combined_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.948944Z","iopub.execute_input":"2025-11-14T03:08:01.949167Z","iopub.status.idle":"2025-11-14T03:08:01.962713Z","shell.execute_reply.started":"2025-11-14T03:08:01.949126Z","shell.execute_reply":"2025-11-14T03:08:01.962121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load... the data?\n\ndef load_data(joined_df, size=(512,512)):\n    images = []\n    masks = []\n    #Find all images by id\n    grouped = joined_df.groupby('ID')\n    for img_id, group in grouped:\n        # Load image\n        image_path = group['ImagePath'].iloc[0]  # all rows have same image path\n        images.append(load_img(image_path, size=size))\n        \n        # Combine all masks for this image\n        masks.append(combine_masks(group, size=size))\n        \n    return np.array(images, dtype=np.float32), np.array(masks, dtype=np.uint8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.963397Z","iopub.execute_input":"2025-11-14T03:08:01.963604Z","iopub.status.idle":"2025-11-14T03:08:01.984731Z","shell.execute_reply.started":"2025-11-14T03:08:01.963589Z","shell.execute_reply":"2025-11-14T03:08:01.983934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Y = load_data(train_df)\nprint(\"Images shape:\", X.shape)\nprint(\"Masks shape:\", Y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:08:01.985406Z","iopub.execute_input":"2025-11-14T03:08:01.985581Z","iopub.status.idle":"2025-11-14T03:09:03.605367Z","shell.execute_reply.started":"2025-11-14T03:08:01.985568Z","shell.execute_reply":"2025-11-14T03:09:03.604349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Define class colors and names\nCLASS_INFO = {\n    1: (\"Microaneurysms\", (255, 0, 0)),      # Red\n    2: (\"Haemorrhages\", (0, 255, 0)),        # Green\n    3: (\"Hard Exudates\", (0, 0, 255)),       # Blue\n    4: (\"Soft Exudates\", (255, 255, 0)),     # Yellow \n}\n\n#Wow so much work just to see a couple eyeball pics \n#I generated this whole function with chatgpt\ndef visualize_image_with_mask(image, mask):\n    \"\"\"\n    image: numpy array (H,W,3), float32 [0,1] RGB\n    mask: numpy array (H,W,1), uint8 class IDs (0 = background)\n    \"\"\"\n    mask = mask.squeeze()\n    overlay = np.zeros_like(image, dtype=np.uint8)\n\n    # Build overlay per class\n    for class_id, (_, color) in CLASS_INFO.items():\n        overlay[mask == class_id] = color\n\n    # Display side by side\n    plt.figure(figsize=(12,6))\n\n    plt.subplot(1,2,1)\n    plt.imshow(image)\n    plt.title(\"Original Image\")\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.imshow(image)\n    plt.imshow(overlay, alpha=0.5)\n    plt.title(\"Image with Mask Overlay\")\n    plt.axis('off')\n\n    # Legend patches\n    patches = [mpatches.Patch(color=np.array(color)/255.0, label=name) \n               for _, (name, color) in CLASS_INFO.items()]\n    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:03.606262Z","iopub.execute_input":"2025-11-14T03:09:03.606485Z","iopub.status.idle":"2025-11-14T03:09:03.616257Z","shell.execute_reply.started":"2025-11-14T03:09:03.606467Z","shell.execute_reply":"2025-11-14T03:09:03.615187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pick a sample from the loaded dataset\nsample_idx = 10\nvisualize_image_with_mask(X[sample_idx], Y[sample_idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:03.617302Z","iopub.execute_input":"2025-11-14T03:09:03.617583Z","iopub.status.idle":"2025-11-14T03:09:04.199565Z","shell.execute_reply.started":"2025-11-14T03:09:03.617557Z","shell.execute_reply":"2025-11-14T03:09:04.198769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now we get to the good shit**","metadata":{}},{"cell_type":"code","source":"#Import torch\nimport torch\nimport numpy as np\n\n#We have to prepare our datasets for pytorch.\n#Why not combine the other function with this one? -some nerd\ndef prepare_for_pytorch(X, Y):\n    # X: (N, H, W, 3) -> (N, 3, H, W)\n    X_torch = np.transpose(X, (0, 3, 1, 2)).astype(np.float32)\n    \n    # Y: (N, H, W, 1) -> (N, H, W), dtype long\n    Y_torch = Y.squeeze(-1).astype(np.int64)\n    \n    # Convert to torch tensors\n    X_torch = torch.from_numpy(X_torch)\n    Y_torch = torch.from_numpy(Y_torch)\n    \n    return X_torch, Y_torch\n\n\nX_torch, Y_torch = prepare_for_pytorch(X, Y)\nprint(\"Torch Images:\", X_torch.shape, X_torch.dtype)  # (N, 3, H, W), float32\nprint(\"Torch Masks:\", Y_torch.shape, Y_torch.dtype)   # (N, H, W), int64\nprint(\"Unique mask values:\", torch.unique(Y_torch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:04.200447Z","iopub.execute_input":"2025-11-14T03:09:04.200703Z","iopub.status.idle":"2025-11-14T03:09:10.595061Z","shell.execute_reply.started":"2025-11-14T03:09:04.200684Z","shell.execute_reply":"2025-11-14T03:09:10.594186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#I think the kernel got turned off at some point so everytime i got a missing import error\n#I added another import line. You can probably fix this up\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom PIL import Image\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#Important for image segmentation, tells us how close the AI mask is to the real one with no background\nclass DiceLoss(nn.Module):\n    \"\"\"\n    Dice Loss for semantic segmentation tasks.\n    \n    Dice Loss (also known as F1 Score Loss or Sørensen–Dice Loss) measures the \n    overlap between predicted and ground truth segmentation masks. It's particularly \n    effective for imbalanced datasets where some classes have much fewer pixels.\n    \n    Formula for single class: Dice = (2 * |X ∩ Y|) / (|X| + |Y|)\n    Dice Loss = 1 - Dice\n    \n    Properties:\n    - Range: [0, 1] where 0 = perfect overlap, 1 = no overlap\n    - Handles class imbalance better than CrossEntropy\n    - Directly optimizes for intersection-over-union (IoU)\n    \"\"\"\n    \n    def __init__(self, smooth=1e-6):\n        \"\"\"\n        Initialize Dice Loss.\n        \n        Args:\n            smooth (float): Smoothing factor to avoid division by zero.\n                           Prevents numerical instability when there are no\n                           pixels of a particular class in either prediction or target.\n        \"\"\"\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth  # Small epsilon for numerical stability\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Compute Dice Loss between predictions and targets.\n        \n        Args:\n            predictions (torch.Tensor): Model outputs with shape (B, C, H, W)\n                                        where C = number of classes.\n                                        Typically comes from final conv layer without softmax.\n            targets (torch.Tensor): Ground truth masks with shape (B, H, W)\n                                    containing class indices (0 to C-1).\n        \n        Returns:\n            torch.Tensor: Scalar Dice Loss value averaged over all non-background classes.\n        \"\"\"\n        # Get number of classes from prediction tensor shape\n        # predictions shape: (batch_size, n_classes, height, width)\n        n_classes = predictions.shape[1]\n        dice_loss = 0  # Accumulator for total dice loss\n        \n        # Convert targets from class indices to one-hot encoding\n        # targets shape: (B, H, W) → (B, H, W, C) → (B, C, H, W)\n        targets_one_hot = F.one_hot(targets, n_classes)  # (B, H, W, C)\n        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n        \n        # Calculate Dice loss for each class (excluding background class 0)\n        for class_id in range(1, n_classes):  # Skip background (class 0)\n            # Get predicted probabilities for current class\n            # Shape: (B, H, W) - model's confidence for this class at each pixel\n            pred = predictions[:, class_id]\n            \n            # Get ground truth binary mask for current class  \n            # Shape: (B, H, W) - 1 where class exists, 0 elsewhere\n            target = targets_one_hot[:, class_id]\n            \n            # Calculate intersection: sum of element-wise product\n            # Measures overlap between prediction and ground truth\n            intersection = (pred * target).sum()\n            \n            # Calculate union: sum of individual areas\n            # Represents total area covered by both prediction and ground truth\n            union = pred.sum() + target.sum()\n            \n            # Compute Dice coefficient with smoothing\n            # Smoothing prevents division by zero when no pixels of this class exist\n            dice = (2. * intersection + self.smooth) / (union + self.smooth)\n            \n            # Convert to loss: 1 - Dice coefficient\n            # We want to maximize Dice, so we minimize (1 - Dice)\n            dice_loss += 1 - dice\n        \n        # Return average Dice loss over all non-background classes\n        # Normalize by (n_classes - 1) because we skipped background\n        return dice_loss / (n_classes - 1)\n\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for addressing class imbalance by down-weighting easy examples\n    and focusing training on hard misclassified examples.\n    \n    Based on: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n    Formula: FL(p_t) = -α * (1 - p_t)^γ * log(p_t)\n    \"\"\"\n    \n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        \"\"\"\n        Args:\n            alpha (float): Weighting factor for class balancing. \n                          α < 1 for majority class, α > 1 for minority class.\n            gamma (float): Focusing parameter. \n                          γ > 0 reduces loss for well-classified examples.\n            reduction (str): 'mean', 'sum', or 'none' for loss reduction.\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs (Tensor): Model predictions (logits), shape (B, C, H, W)\n            targets (Tensor): Ground truth labels, shape (B, H, W)\n        \n        Returns:\n            Tensor: Computed focal loss\n        \"\"\"\n        # Standard cross-entropy loss (pixel-wise)\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        \n        # p_t = probability of true class → higher for confident correct predictions\n        pt = torch.exp(-ce_loss)\n        \n        # Focal loss: down-weight easy examples by (1-p_t)^gamma\n        # - Well-classified (p_t → 1): loss reduced significantly\n        # - Misclassified (p_t → 0): loss largely unchanged\n        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n        \n        # Apply reduction\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n#Combine them. Dice weight tells it how much to weigh the dice loss btw.\nclass CombinedLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, dice_weight=0.7):\n        super(CombinedLoss, self).__init__()\n        self.focal = FocalLoss(alpha=alpha, gamma=gamma)\n        self.dice = DiceLoss()\n        self.dice_weight = dice_weight\n\n    def forward(self, inputs, targets):\n        focal_loss = self.focal(inputs, targets)\n        dice_loss = self.dice(inputs, targets)\n        return (1 - self.dice_weight) * focal_loss + self.dice_weight * dice_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:10.595935Z","iopub.execute_input":"2025-11-14T03:09:10.596186Z","iopub.status.idle":"2025-11-14T03:09:10.608934Z","shell.execute_reply.started":"2025-11-14T03:09:10.596167Z","shell.execute_reply":"2025-11-14T03:09:10.608158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Just finds the class of the anomalies through their names\ndef infer_class_from_path(mask_path):\n    path = mask_path.replace(\"\\\\\", \"/\").lower()\n    filename = os.path.basename(path)\n\n    # 1. DDR folder-based detection (critical!)\n    if '/ma/' in path.lower():\n        return 1\n    if '/he/' in path.lower():\n        return 2\n    if '/ex/' in path.lower():\n        return 3\n    if '/se/' in path.lower():\n        return 4\n\n    # 2. IDRiD folder-based\n    if '/1. microaneurysms/' in path:\n        return 1\n    if '/2. haemorrhages/' in path:\n        return 2\n    if '/3. hard exudates/' in path:\n        return 3\n    if '/4. soft exudates/' in path:\n        return 4\n    if '/5. optic disc/' in path:\n        return 5\n\n    # 3. IDRiD filename-based (suffix patterns)\n    if '_ma' in filename:\n        return 1\n    if '_he' in filename:\n        return 2\n    if '_ex' in filename:\n        return 3\n    if '_se' in filename:\n        return 4\n    if '_od' in filename:\n        return 5\n\n    print(f\"Warning: Could not infer class from path: {mask_path}\")\n    return 1\n\ndef _get_class_name(class_id):\n    \"\"\"Get class name from class ID\"\"\"\n    class_names = {\n        1: \"Microaneurysms\",\n        2: \"Haemorrhages\", \n        3: \"Hard Exudates\",\n        4: \"Soft Exudates\",\n        5: \"Optic Disc\"\n    }\n    return class_names.get(class_id, \"Unknown\")\n\ndef combine_masks_by_image(df, img_col='ImagePath', mask_col='MaskPath'):\n    \"\"\"Group DataFrame by image and combine all masks for each image\"\"\"\n    print(\"Combining masks by image...\")\n    \n    # Group by image and collect all mask paths\n    grouped = df.groupby(img_col)[mask_col].apply(list).reset_index()\n    \n    combined_data = []\n    for _, row in grouped.iterrows():\n        img_path = row[img_col]\n        mask_paths = row[mask_col]\n        \n        # Create a dictionary for this image with all its masks\n        img_data = {'ImagePath': img_path}\n        \n        # Classify each mask path\n        for mask_path in mask_paths:\n            class_id = infer_class_from_path(mask_path)\n            class_name = _get_class_name(class_id).replace(' ', '')\n            img_data[f'{class_name}_path'] = mask_path\n        \n        combined_data.append(img_data)\n    \n    combined_df = pd.DataFrame(combined_data)\n    print(f\"Combined {len(df)} rows -> {len(combined_df)} images\")\n    return combined_df\n\n#Combine masks for the data\nclass RetinopathyCombinedDatasetSimple(Dataset):\n    def __init__(self, df, img_col='ImagePath', mask_dict=None, size=(512, 512), augment=False):\n        self.df = df.copy()\n        self.img_col = img_col\n        self.mask_dict = mask_dict or {\n            \"Microaneurysms\": \"Microaneurysms_path\",\n            \"Haemorrhages\": \"Haemorrhages_path\", \n            \"Hard Exudates\": \"HardExudates_path\",\n            \"Soft Exudates\": \"SoftExudates_path\",\n            \"Optic Disc\": \"OpticDisc_path\"\n        }\n        self.size = size\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # Load image\n        image = cv2.imread(row[self.img_col])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Resize image to target size\n        image = cv2.resize(image, (self.size[1], self.size[0]))\n        \n        # Create empty mask\n        combined_mask = np.zeros((self.size[0], self.size[1]), dtype=np.uint8)\n        \n        # Load and add each class\n        for class_id, (class_name, mask_col) in enumerate(self.mask_dict.items(), start=1):\n            mask_path = row[mask_col]\n            \n            if pd.isna(mask_path) or mask_path == '':\n                continue\n                \n            try:\n                class_mask = np.array(Image.open(mask_path))\n                if len(class_mask.shape) == 3:\n                    class_mask = class_mask[:, :, 0]\n                \n                # Resize to target size\n                class_mask = cv2.resize(class_mask, (self.size[1], self.size[0]), \n                                      interpolation=cv2.INTER_NEAREST)\n                \n                # Add to combined mask\n                combined_mask[class_mask > 0] = class_id\n                \n            except Exception as e:\n                print(f\"Error loading {class_name}: {e}\")\n                continue\n        \n        # Convert to tensor\n        image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n        mask_tensor = torch.from_numpy(combined_mask).long()\n        \n        return image_tensor, mask_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:10.609802Z","iopub.execute_input":"2025-11-14T03:09:10.609990Z","iopub.status.idle":"2025-11-14T03:09:10.634620Z","shell.execute_reply.started":"2025-11-14T03:09:10.609976Z","shell.execute_reply":"2025-11-14T03:09:10.633877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**MODEL ARCHITECTURE**","metadata":{}},{"cell_type":"code","source":"class ConvBNReLU(nn.Module):\n    def __init__(self, n_channels, out_ch, k=3, stride=1, padding=None, groups=1):\n        super().__init__()\n        if padding is None:\n            padding = (k - 1) // 2\n        self.conv = nn.Conv2d(n_channels, out_ch, k, stride=stride, padding=padding, groups=groups, bias=False)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.act = nn.GELU()\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\nclass DepthwiseSeparable(nn.Module):\n    def __init__(self, n_channels, out_ch, k=3, stride=1):\n        super().__init__()\n        pad = (k-1)//2\n        self.dw = nn.Conv2d(n_channels, n_channels, kernel_size=k, stride=stride, padding=pad, groups=n_channels, bias=False)\n        self.pw = nn.Conv2d(n_channels, out_ch, kernel_size=1, bias=False)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.act = nn.GELU()\n    def forward(self, x):\n        x = self.dw(x)\n        x = self.pw(x)\n        x = self.bn(x)\n        return self.act(x)\n\n# -------------------------\n# Polarized Self-Attention (PSA)\n# -------------------------\nclass PolarizedSelfAttention(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.channels = channels\n        \n        self.channel_pool = nn.AdaptiveAvgPool2d(1)\n        self.channel_fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.GELU(),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n        \n        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n        self.spatial_sigmoid = nn.Sigmoid()\n    \n    def channel_attention(self, x):\n        b, c, _, _ = x.size()\n        y = self.channel_pool(x).view(b, c)\n        y = self.channel_fc(y).view(b, c, 1, 1)\n        return x * y\n    \n    def spatial_attention(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        y = torch.cat([avg_out, max_out], dim=1)\n        y = self.spatial_conv(y)\n        y = self.spatial_sigmoid(y)\n        return x * y\n    \n    def forward(self, x):\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        return x\n\n# -------------------------\n# Mobile Attention Convolution (MAC)\n# -------------------------\nclass MAC(nn.Module):\n    def __init__(self, n_channels, out_ch, expansion=4):\n        super().__init__()\n        hidden_dim = n_channels * expansion\n        \n        self.expand = nn.Conv2d(n_channels, hidden_dim, 1, bias=False) if n_channels != hidden_dim else nn.Identity()\n        self.bn1 = nn.BatchNorm2d(hidden_dim)\n        \n        self.dw_conv = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, groups=hidden_dim, bias=False)\n        self.bn2 = nn.BatchNorm2d(hidden_dim)\n        \n        self.psa = PolarizedSelfAttention(hidden_dim)\n        \n        self.project = nn.Conv2d(hidden_dim, out_ch, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_ch)\n        \n        self.act = nn.GELU()\n        self.use_residual = (n_channels == out_ch)\n    \n    def forward(self, x):\n        identity = x\n        \n        x = self.expand(x)\n        x = self.bn1(x)\n        x = self.act(x)\n        \n        x = self.dw_conv(x)\n        x = self.bn2(x)\n        x = self.act(x)\n        \n        x = self.psa(x)\n        \n        x = self.project(x)\n        x = self.bn3(x)\n        \n        if self.use_residual:\n            x = x + identity\n            \n        return self.act(x)\n\n# -------------------------\n# Multi-scale Large-kernel Dual Attention (MLDA)\n# -------------------------\nclass MLDA(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        \n        self.local_conv = nn.Conv2d(channels, channels, 1, bias=False)\n        self.local_bn = nn.BatchNorm2d(channels)\n        \n        self.channel_att = PolarizedSelfAttention(channels)\n        \n        self.branch0 = DepthwiseSeparable(channels, channels, k=5)\n        \n        self.branch1 = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=(7, 1), padding=(3, 0), groups=channels, bias=False),\n            nn.Conv2d(channels, channels, kernel_size=(1, 7), padding=(0, 3), groups=channels, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.GELU()\n        )\n        \n        self.branch2 = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=(11, 1), padding=(5, 0), groups=channels, bias=False),\n            nn.Conv2d(channels, channels, kernel_size=(1, 11), padding=(0, 5), groups=channels, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.GELU()\n        )\n        \n        self.branch3 = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=(21, 1), padding=(10, 0), groups=channels, bias=False),\n            nn.Conv2d(channels, channels, kernel_size=(1, 21), padding=(0, 10), groups=channels, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.GELU()\n        )\n        \n        self.fuse = nn.Conv2d(channels * 4, channels, 1, bias=False)\n        self.fuse_bn = nn.BatchNorm2d(channels)\n        self.fuse_act = nn.GELU()\n        \n        self.final_conv = nn.Conv2d(channels, channels, 1, bias=False)\n        \n    def forward(self, x):\n        local = self.local_conv(x)\n        local = self.local_bn(local)\n        \n        x_att = self.channel_att(local)\n        \n        b0 = self.branch0(x_att)\n        b1 = self.branch1(x_att)\n        b2 = self.branch2(x_att)\n        b3 = self.branch3(x_att)\n        \n        multi_scale = torch.cat([b0, b1, b2, b3], dim=1)\n        fused = self.fuse(multi_scale)\n        fused = self.fuse_bn(fused)\n        fused = self.fuse_act(fused)\n        \n        out = self.final_conv(fused)\n        return out\n\n# -------------------------\n# Mobile Multi-scale Attention Convolution (MMAC)\n# -------------------------\nclass MMAC(nn.Module):\n    def __init__(self, n_channels, out_ch):\n        super().__init__()\n        self.mac1 = MAC(n_channels, out_ch)\n        self.mac2 = MAC(out_ch, out_ch)\n        self.mlda = MLDA(out_ch)\n        \n    def forward(self, x):\n        x = self.mac1(x)\n        x = self.mac2(x)\n        x = self.mlda(x)\n        return x\n\n# -------------------------\n# CPCF fusion module - FIXED VERSION\n# -------------------------\nclass ResizeOp(nn.Module):\n    \"\"\"Learnable resize operation with projection to target channels\"\"\"\n    def __init__(self, n_channels, out_ch, mode='up'):\n        super().__init__()\n        self.mode = mode\n        self.conv = nn.Conv2d(n_channels, out_ch, 1, bias=False)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        if self.mode == 'up':\n            return F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        else:\n            return F.avg_pool2d(x, kernel_size=2, stride=2)\n\nclass CPCFModule(nn.Module):\n    \"\"\"\n    Fixed CPCF module with proper channel projection\n    \"\"\"\n    def __init__(self, target_channels, stage, f1_ch, f2_ch, f3_ch, f4_ch):\n        super().__init__()\n        self.stage = stage\n        self.target_channels = target_channels\n        \n        # Projection layers for each input feature to target_channels\n        self.proj1 = nn.Conv2d(f1_ch, target_channels, 1, bias=False)\n        self.proj2 = nn.Conv2d(f2_ch, target_channels, 1, bias=False)\n        self.proj3 = nn.Conv2d(f3_ch, target_channels, 1, bias=False)\n        self.proj4 = nn.Conv2d(f4_ch, target_channels, 1, bias=False)\n        \n        # Resize operations\n        self.rup = ResizeOp(target_channels, target_channels, mode='up')\n        self.rdown = ResizeOp(target_channels, target_channels, mode='down')\n        \n        # Attention block\n        self.att = MLDA(target_channels)\n        \n    def forward(self, f1, f2, f3, f4):\n        # Project all features to target channel dimension\n        f1_p = self.proj1(f1)\n        f2_p = self.proj2(f2)\n        f3_p = self.proj3(f3)\n        f4_p = self.proj4(f4)\n        \n        # Implement the exact fusion strategy from paper Eq.(3)\n        if self.stage == 1:\n            # S1 = Rup(Rup(Rup(f4) + f3) + f2) + f1\n            s = self.rup(self.rup(self.rup(f4_p) + f3_p) + f2_p) + f1_p\n            ref_feature = f1_p\n        elif self.stage == 2:\n            # S2 = Rup(Rup(f4) + f3) + Rdown(f1) + f2\n            s = self.rup(self.rup(f4_p) + f3_p) + self.rdown(f1_p) + f2_p\n            ref_feature = f2_p\n        elif self.stage == 3:\n            # S3 = Rdown(Rdown(f1) + f2) + Rup(f4) + f3\n            s = self.rdown(self.rdown(f1_p) + f2_p) + self.rup(f4_p) + f3_p\n            ref_feature = f3_p\n        elif self.stage == 4:\n            # S4 = Rdown(Rdown(Rdown(f1) + f2) + f3) + f4\n            s = self.rdown(self.rdown(self.rdown(f1_p) + f2_p) + f3_p) + f4_p\n            ref_feature = f4_p\n        \n        # Apply attention and skip connection\n        m = self.att(s) + ref_feature\n        return m\n\n# -------------------------\n# Complete CMAC-Net\n# -------------------------\nclass CMACNet(nn.Module):\n    def __init__(self, n_channels=3, base_ch=32, n_classes=5):\n        super().__init__()\n        \n        # -------------------------\n        # Encoder (MDAC Backbone)\n        # -------------------------\n        self.stem = nn.Sequential(\n            nn.Conv2d(n_channels, base_ch, 3, stride=2, padding=1),\n            nn.Conv2d(base_ch, base_ch, 3, stride=2, padding=1),\n            nn.BatchNorm2d(base_ch),\n            nn.GELU()\n        )\n        \n        # Stage 1\n        self.stage1 = nn.Sequential(MAC(base_ch, base_ch))\n        self.pool1 = nn.MaxPool2d(2)\n        \n        # Stage 2\n        self.stage2 = nn.Sequential(\n            MAC(base_ch, base_ch * 2),\n            MAC(base_ch * 2, base_ch * 2)\n        )\n        self.pool2 = nn.MaxPool2d(2)\n        \n        # Stage 3\n        self.stage3 = nn.Sequential(\n            MMAC(base_ch * 2, base_ch * 4),\n            MMAC(base_ch * 4, base_ch * 4),\n            MMAC(base_ch * 4, base_ch * 4)\n        )\n        self.pool3 = nn.MaxPool2d(2)\n        \n        # Stage 4\n        self.stage4 = nn.Sequential(\n            MMAC(base_ch * 4, base_ch * 8),\n            MMAC(base_ch * 8, base_ch * 8),\n            MMAC(base_ch * 8, base_ch * 8),\n            MMAC(base_ch * 8, base_ch * 8),\n            MMAC(base_ch * 8, base_ch * 8),\n            MMAC(base_ch * 8, base_ch * 8)\n        )\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Stage 5 (bottleneck)\n        self.stage5 = nn.Sequential(\n            MMAC(base_ch * 8, base_ch * 16),\n            MMAC(base_ch * 16, base_ch * 16),\n            MMAC(base_ch * 16, base_ch * 16)\n        )\n        \n        # -------------------------\n        # CPCF Modules with proper channel handling\n        # -------------------------\n        # Define the actual channel dimensions for each stage\n        f1_ch = base_ch      # Stage 1 output channels\n        f2_ch = base_ch * 2  # Stage 2 output channels  \n        f3_ch = base_ch * 4  # Stage 3 output channels\n        f4_ch = base_ch * 8  # Stage 4 output channels\n        \n        self.cpcf1 = CPCFModule(f1_ch, stage=1, f1_ch=f1_ch, f2_ch=f2_ch, f3_ch=f3_ch, f4_ch=f4_ch)\n        self.cpcf2 = CPCFModule(f2_ch, stage=2, f1_ch=f1_ch, f2_ch=f2_ch, f3_ch=f3_ch, f4_ch=f4_ch)\n        self.cpcf3 = CPCFModule(f3_ch, stage=3, f1_ch=f1_ch, f2_ch=f2_ch, f3_ch=f3_ch, f4_ch=f4_ch)\n        self.cpcf4 = CPCFModule(f4_ch, stage=4, f1_ch=f1_ch, f2_ch=f2_ch, f3_ch=f3_ch, f4_ch=f4_ch)\n        \n        # -------------------------\n        # Decoder\n        # -------------------------\n        self.up5 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBNReLU(base_ch * 16, base_ch * 8, k=3) \n        )\n        \n        self.up4 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBNReLU(base_ch * 8, base_ch * 4, k=3) \n        )\n        \n        self.up3 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBNReLU(base_ch * 4, base_ch * 2, k=3)  \n        )\n        \n        self.up2 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBNReLU(base_ch * 2, base_ch, k=3)  \n        )\n        \n        self.up1 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBNReLU(base_ch, base_ch, k=3)\n        )\n        \n        # Final output\n        self.final_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.final_conv = nn.Conv2d(base_ch, n_classes, 1)\n        \n    def forward(self, x):\n        # Initial stem\n        x = self.stem(x)\n        \n        # Encoder forward pass\n        f1 = self.stage1(x)           # base_ch\n        f2 = self.stage2(self.pool1(f1))  # base_ch * 2\n        f3 = self.stage3(self.pool2(f2))  # base_ch * 4\n        f4 = self.stage4(self.pool3(f3))  # base_ch * 8\n        f5 = self.stage5(self.pool4(f4))  # base_ch * 16\n        \n        # CPCF fusion\n        m1 = self.cpcf1(f1, f2, f3, f4)\n        m2 = self.cpcf2(f1, f2, f3, f4)\n        m3 = self.cpcf3(f1, f2, f3, f4)\n        m4 = self.cpcf4(f1, f2, f3, f4)\n        \n        # Decoder with skip connections\n        d4 = self.up5(f5) + m4\n        d3 = self.up4(d4) + m3\n        d2 = self.up3(d3) + m2\n        d1 = self.up2(d2) + m1\n        d0 = self.up1(d1)\n        \n        # Final output\n        out = self.final_upsample(d0)\n        out = self.final_conv(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:10.635461Z","iopub.execute_input":"2025-11-14T03:09:10.635687Z","iopub.status.idle":"2025-11-14T03:09:10.673307Z","shell.execute_reply.started":"2025-11-14T03:09:10.635671Z","shell.execute_reply":"2025-11-14T03:09:10.672755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming your cmac model is defined as `model`\nn_classes = len(CLASS_NAMES) + 1  # background + subclasses\nmodel = CMACNet(n_channels=3, n_classes=n_classes)  # initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:10.673955Z","iopub.execute_input":"2025-11-14T03:09:10.674188Z","iopub.status.idle":"2025-11-14T03:09:11.510709Z","shell.execute_reply.started":"2025-11-14T03:09:10.674165Z","shell.execute_reply":"2025-11-14T03:09:11.510113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iou_per_class(predictions, targets, n_classes=6):\n    \"\"\"Calculate IoU for each class separately\"\"\"\n    ious = []\n    predictions = torch.argmax(predictions, dim=1)\n    \n    for class_id in range(1, n_classes):  # Skip background\n        pred_mask = (predictions == class_id)\n        target_mask = (targets == class_id)\n        \n        intersection = (pred_mask & target_mask).float().sum()\n        union = (pred_mask | target_mask).float().sum()\n        \n        if union > 0:\n            iou = intersection / union\n        else:\n            iou = torch.tensor(0.0)\n        ious.append(iou.item())\n    \n    return ious\n\ndef create_weighted_sampler(dataset):\n    \"\"\"Create sampler that oversamples images with rare pathologies\"\"\"\n    weights = []\n    \n    for i, (_, mask) in enumerate(dataset):\n        unique_classes = torch.unique(mask)\n        \n        # Higher weight if image contains rare pathologies (classes 1-4)\n        rare_classes_present = any(cls in unique_classes for cls in [1, 2, 3, 4])\n        weight = 5.0 if rare_classes_present else 1.0\n        weights.append(weight)\n    \n    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n    return sampler\n\ndef train_one_epoch_with_gradient_control(model, dataloader, optimizer, criterion, device, n_classes=6, max_grad_norm=1.0):\n    model.train()\n    running_loss = 0.0\n    class_ious = [[] for _ in range(n_classes-1)]\n    gradient_norms = []\n    \n    loop = tqdm(dataloader, leave=False)\n    for imgs, masks in loop:\n        imgs, masks = imgs.to(device), masks.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        \n        # Gradient clipping\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        gradient_norms.append(grad_norm.item())\n        \n        optimizer.step()\n\n        # Calculate metrics\n        batch_ious = calculate_iou_per_class(outputs, masks, n_classes)\n        for i, iou in enumerate(batch_ious):\n            class_ious[i].append(iou)\n\n        running_loss += loss.item()\n        loop.set_description(f\"Training (Grad: {grad_norm:.2f})\")\n        loop.set_postfix(loss=loss.item())\n    \n    # Print metrics\n    avg_ious = [np.mean(iou_list) for iou_list in class_ious]\n    class_names = [\"MA\", \"HE\", \"EX\", \"SE\", \"OD\"]\n    print(\"Per-class IoU:\", {name: f\"{iou:.3f}\" for name, iou in zip(class_names, avg_ious)})\n    print(f\"Avg Gradient Norm: {np.mean(gradient_norms):.3f}\")\n    \n    return running_loss / len(dataloader)\n\n#Evaluate model performance\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for imgs, masks in dataloader:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n            val_loss += loss.item()\n    return val_loss / len(dataloader)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Enhanced training function with visualization\ndef train_model_enhanced(model, train_loader, val_loader, epochs, device, class_names, checkpoint_path=\"cmac_best_balanced.pth\"):\n    best_val_loss = float(\"inf\")\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)  # Higher LR, AdamW\n\n# Warmup scheduler\ndef get_warmup_scheduler(optimizer, warmup_epochs=10):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(warmup_epochs)\n        else:\n            return 0.1  # Then use ReduceLROnPlateau\n            return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n        warmup_scheduler = get_warmup_scheduler(optimizer, warmup_epochs=10)\n        plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=8, factor=0.5, min_lr=1e-7)\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(\"=\" * 50)\n        \n        # Train with metrics\n        train_loss = train_one_epoch_with_metrics(model, train_loader, optimizer, criterion, device)\n        val_loss = evaluate(model, val_loader, criterion, device)\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"✅ Saved best model | Val Loss: {val_loss:.4f}\")\n\n        # Visualize predictions after each epoch\n        print(f\"\\n📊 Visualization for Epoch {epoch+1}:\")\n        visualize_predictions(model, val_loader, class_names, num_samples=3)\n\n# Also add a function to show class color mapping\ndef show_class_legend():\n    \"\"\"Show color mapping for each class\"\"\"\n    class_info = {\n        0: (\"Background\", \"black\"),\n        1: (\"Microaneurysms\", \"red\"),\n        2: (\"Haemorrhages\", \"green\"), \n        3: (\"Hard Exudates\", \"blue\"),\n        4: (\"Soft Exudates\", \"yellow\"),\n        5: (\"Optic Disc\", \"magenta\")\n    }\n    \n    plt.figure(figsize=(8, 2))\n    for i in range(6):\n        plt.subplot(1, 6, i+1)\n        color = plt.cm.tab10(i / 10)  # Using tab10 colormap\n        plt.imshow([[i]], vmin=0, vmax=5, cmap=\"tab10\")\n        plt.title(f\"{i}: {class_info[i][0]}\", fontsize=8)\n        plt.axis('off')\n    plt.suptitle(\"Class Color Mapping\", fontsize=12)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:11.511547Z","iopub.execute_input":"2025-11-14T03:09:11.511822Z","iopub.status.idle":"2025-11-14T03:09:14.801298Z","shell.execute_reply.started":"2025-11-14T03:09:11.511804Z","shell.execute_reply":"2025-11-14T03:09:14.800689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Combine your data\ncombined_df = combine_masks_by_image(train_df)\nprint(\"\\nCombined DataFrame columns:\", combined_df.columns.tolist())\n\n# 2. Create datasets\ntrain_dataset = RetinopathyCombinedDatasetSimple(\n    combined_df, \n    img_col='ImagePath',\n    mask_dict={\n        \"Microaneurysms\": \"Microaneurysms_path\",\n        \"Haemorrhages\": \"Haemorrhages_path\",\n        \"Hard Exudates\": \"HardExudates_path\", \n        \"Soft Exudates\": \"SoftExudates_path\",\n        \"Optic Disc\": \"OpticDisc_path\"\n    },\n    size=(512, 512),\n    augment=False\n)\n\nval_dataset = RetinopathyCombinedDatasetSimple(\n    combined_df, \n    img_col='ImagePath',\n    mask_dict={\n        \"Microaneurysms\": \"Microaneurysms_path\",\n        \"Haemorrhages\": \"Haemorrhages_path\", \n        \"Hard Exudates\": \"HardExudates_path\",\n        \"Soft Exudates\": \"SoftExudates_path\",\n        \"Optic Disc\": \"OpticDisc_path\"\n    },\n    size=(512, 512),\n    augment=False\n)\n\n# 3. Create weighted sampler for training\ntrain_sampler = create_weighted_sampler(train_dataset)\ntrain_loader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n\n# 4. Setup model and training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_classes = 6  # background + 5 classes\nmodel = CMACNet(n_channels=3, n_classes=n_classes).to(device)\n\n# 5. Use balanced loss function\ncriterion = CombinedLoss(alpha=1, gamma=2, dice_weight=0.7)\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n\nprint(f\"Training on: {device}\")\nprint(f\"Model configured for {n_classes} classes\")\nprint(\"Using Combined Loss (Focal + Dice) for better class balance\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:09:14.802091Z","iopub.execute_input":"2025-11-14T03:09:14.802630Z","iopub.status.idle":"2025-11-14T03:10:06.018391Z","shell.execute_reply.started":"2025-11-14T03:09:14.802602Z","shell.execute_reply":"2025-11-14T03:10:06.017354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update your main execution section:\n# Define class names for visualization\nCLASS_NAMES = [\"Background\", \"Microaneurysms\", \"Haemorrhages\", \"Hard Exudates\", \"Soft Exudates\", \"Optic Disc\"]\n\n# Show class legend before training\nprint(\"Class Color Mapping:\")\nshow_class_legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:10:06.021691Z","iopub.execute_input":"2025-11-14T03:10:06.021952Z","iopub.status.idle":"2025-11-14T03:10:06.346889Z","shell.execute_reply.started":"2025-11-14T03:10:06.021928Z","shell.execute_reply":"2025-11-14T03:10:06.346115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for imgs, masks in train_loader:\n    print(\"Batch images:\", imgs.shape)  # (B, 3, H, W)\n    print(\"Batch masks:\", masks.shape)  # (B, H, W)\n    print(\"Unique mask labels:\", torch.unique(masks))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:41:26.475600Z","iopub.status.idle":"2025-11-14T03:41:26.475917Z","shell.execute_reply.started":"2025-11-14T03:41:26.475738Z","shell.execute_reply":"2025-11-14T03:41:26.475756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n#Just a visualizer to see how the model performs after epoch\ndef visualize_predictions(model, dataloader, class_names, num_samples=4):\n    model.eval()\n    imgs, masks = next(iter(dataloader))  # take one batch\n    imgs, masks = imgs.to(device), masks.to(device)\n\n    with torch.no_grad():\n        outputs = model(imgs)  # (B, C, H, W)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()  # take class with max logit\n\n    imgs = imgs.cpu().numpy().transpose(0, 2, 3, 1)  # back to (B, H, W, C)\n    masks = masks.cpu().numpy()\n\n    plt.figure(figsize=(12, num_samples * 4))\n    for i in range(num_samples):\n        # Original image\n        plt.subplot(num_samples, 3, i*3 + 1)\n        plt.imshow(imgs[i])\n        plt.title(\"Image\")\n        plt.axis(\"off\")\n\n        # Ground truth\n        plt.subplot(num_samples, 3, i*3 + 2)\n        plt.imshow(masks[i], cmap=\"tab20\")\n        plt.title(\"Ground Truth\")\n        plt.axis(\"off\")\n\n        # Prediction\n        plt.subplot(num_samples, 3, i*3 + 3)\n        plt.imshow(preds[i], cmap=\"tab20\")\n        plt.title(\"Prediction\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:10:06.993717Z","iopub.execute_input":"2025-11-14T03:10:06.993944Z","iopub.status.idle":"2025-11-14T03:10:07.002496Z","shell.execute_reply.started":"2025-11-14T03:10:06.993928Z","shell.execute_reply":"2025-11-14T03:10:07.001695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def debug_predictions(model, dataloader, device):\n    \"\"\"Debug what the model is actually predicting\"\"\"\n    model.eval()\n    imgs, masks = next(iter(dataloader))\n    imgs, masks = imgs.to(device), masks.to(device)\n    \n    with torch.no_grad():\n        outputs = model(imgs)\n        preds = torch.argmax(outputs, dim=1)\n        probabilities = F.softmax(outputs, dim=1)\n    \n    print(\"\\n🔍 PREDICTION DEBUG:\")\n    print(f\"Output range: [{outputs.min().item():.3f}, {outputs.max().item():.3f}]\")\n    \n    for class_id in range(6):\n        class_probs = probabilities[:, class_id]\n        class_preds = (preds == class_id).float()\n        print(f\"Class {class_id}: Avg prob {class_probs.mean().item():.3f}, Pred % {class_preds.mean().item():.3f}\")\n    \n    # Check if any class gets reasonable predictions\n    if (preds == 0).float().mean() > 0.95:\n        print(\"🚨 WARNING: Model predicting mostly background!\")\n    elif preds.float().std() < 0.01:\n        print(\"🚨 WARNING: Model predictions have very low variance!\")\n\n# Add this to your training loop\ndebug_predictions(model, val_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:10:07.003678Z","iopub.execute_input":"2025-11-14T03:10:07.003915Z","iopub.status.idle":"2025-11-14T03:10:07.968436Z","shell.execute_reply.started":"2025-11-14T03:10:07.003890Z","shell.execute_reply":"2025-11-14T03:10:07.967489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model_fixed(model, train_loader, val_loader, epochs, device, class_names):\n    # Start simple\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        print(f\"\\n🎯 Epoch {epoch+1}/{epochs}\")\n        print(\"=\" * 60)\n        \n        # Debug before training\n        if epoch == 0:\n            print(\"Initial model state:\")\n            debug_predictions(model, val_loader, device)\n        \n        # Train\n        train_loss = train_one_epoch_with_gradient_control(\n            model, train_loader, optimizer, criterion, device, \n            max_grad_norm=1.0\n        )\n        \n        # Validate\n        val_loss = evaluate(model, val_loader, criterion, device)\n        scheduler.step(val_loss)\n        \n        print(f\"📊 Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n        print(f\"📈 Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"cmac_best.pth\")\n            print(f\"✅ Saved best model | Val Loss: {val_loss:.4f}\")\n        \n        # Debug predictions every few epochs\n        if epoch % 5 == 0:\n            debug_predictions(model, val_loader, device)\n        \n        # Visualize\n        if epoch % 2 == 0:\n            print(f\"\\n👀 Visualization Epoch {epoch+1}:\")\n            visualize_predictions(model, val_loader, class_names, num_samples=2)\n        \n        # If no progress after 10 epochs, try different approach\n        if epoch > 10 and best_val_loss > 2.0:  # If loss is still very high\n            print(\"🔄 No significant progress, trying weighted loss...\")\n            real_weights = calculate_class_weights_from_data(train_dataset)\n            criterion = nn.CrossEntropyLoss(weight=real_weights.to(device))\n\n# Reset model and start fresh\ndef reset_model():\n    model = CMACNet(n_channels=3, n_classes=6).to(device)\n    \n    # Initialize weights properly\n    def init_weights(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    \n    model.apply(init_weights)\n    return model\n\n# Start fresh\n#print(\"🔄 Starting fresh with proper initialization...\")\n#model = reset_model()\ntrain_model_fixed(model, train_loader, val_loader, epochs=200, device=device, class_names=CLASS_NAMES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:10:07.969313Z","iopub.execute_input":"2025-11-14T03:10:07.969554Z","iopub.status.idle":"2025-11-14T03:41:26.467732Z","shell.execute_reply.started":"2025-11-14T03:10:07.969525Z","shell.execute_reply":"2025-11-14T03:41:26.466454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save\ntorch.save(model.state_dict(), \"CMAC_weights.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T03:41:26.468362Z","iopub.status.idle":"2025-11-14T03:41:26.468669Z","shell.execute_reply.started":"2025-11-14T03:41:26.468508Z","shell.execute_reply":"2025-11-14T03:41:26.468519Z"}},"outputs":[],"execution_count":null}]}